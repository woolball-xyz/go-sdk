/*
 * Woolball AI Network API
 *
 * **Transform idle browsers into a powerful distributed AI inference network**  For detailed examples and model lists, visit our [GitHub repository](https://github.com/woolball-xyz/woolball-server).
 *
 * API version: v1
 * Generated by: Swagger Codegen (https://github.com/swagger-api/swagger-codegen.git)
 */
package swagger

type TextGenerationRequestContract struct {
	// The AI provider to use
	Provider string `json:"provider"`
	// The AI model to use for processing
	Model string `json:"model"`
	// Input text or messages for generation
	Input string `json:"input"`
	// The number of highest probability vocabulary tokens to keep for top-k-filtering
	TopK int32 `json:"top_k,omitempty"`
	// If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation
	TopP float64 `json:"top_p,omitempty"`
	// The value used to modulate the next token probabilities
	Temperature float64 `json:"temperature,omitempty"`
	// Parameter for repetition penalty. 1.0 means no penalty
	RepetitionPenalty float64 `json:"repetition_penalty,omitempty"`
	// Quantization level (e.g., 'fp16', 'q4', 'q8') - Transformers only
	Dtype string `json:"dtype,omitempty"`
	// Maximum length the generated tokens can have - Transformers only
	MaxLength int32 `json:"max_length,omitempty"`
	// Maximum number of tokens to generate - Transformers only
	MaxNewTokens int32 `json:"max_new_tokens,omitempty"`
	// Minimum length of the sequence to be generated - Transformers only
	MinLength int32 `json:"min_length,omitempty"`
	// Minimum numbers of tokens to generate - Transformers only
	MinNewTokens int32 `json:"min_new_tokens,omitempty"`
	// Whether to use sampling - Transformers only
	DoSample bool `json:"do_sample,omitempty"`
	// Number of beams for beam search - Transformers only
	NumBeams int32 `json:"num_beams,omitempty"`
	// If > 0, all ngrams of that size can only occur once - Transformers only
	NoRepeatNgramSize int32 `json:"no_repeat_ngram_size,omitempty"`
	// Size of the context window for the model - WebLLM only
	ContextWindowSize int32 `json:"context_window_size,omitempty"`
	// Size of the sliding window for attention - WebLLM only
	SlidingWindowSize int32 `json:"sliding_window_size,omitempty"`
	// Size of the attention sink - WebLLM only
	AttentionSinkSize int32 `json:"attention_sink_size,omitempty"`
	// Penalty for token frequency - WebLLM only
	FrequencyPenalty float64 `json:"frequency_penalty,omitempty"`
	// Penalty for token presence - WebLLM only
	PresencePenalty float64 `json:"presence_penalty,omitempty"`
	// Beginning of sequence token ID - WebLLM only
	BosTokenId int32 `json:"bos_token_id,omitempty"`
	// Maximum number of tokens to generate - MediaPipe only
	MaxTokens int32 `json:"max_tokens,omitempty"`
	// Random seed for reproducible results - MediaPipe only
	RandomSeed int32 `json:"random_seed,omitempty"`
}
